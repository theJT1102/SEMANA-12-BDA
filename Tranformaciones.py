# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F7hwM1hA6pipLqupUIyNfPCjbX5SSzd5
"""

pip install pyspark

import pyspark

from pyspark import SparkContext

emp_data = [1, 2, 36, 8, 2, 7, 1, 8, 2]
rdd = spark.sparkContext.parallelize(emp_data)

#map
rdd2 = emp_df.rdd
mapped_rdd = rdd.map(lambda x: (x.employee_id, x.name, int(x.salary) /2))
print(mapped_rdd.collect())

#filter
rdd = emp_df.rdd
filtered_rdd = rdd.filter(lambda x:  int(x.age) < 27)
print(filtered_rdd.collect())

#flatmap
flatmapped_rdd = rdd.flatMap(lambda x: x.name.split(" "))
print(flatmapped_rdd.collect())

#union
additional_data = [
 ("021", "108", "Lucas Black", "29", "Male", "53000", "2017-07-01"),
 ("022", "109", "Natalie Brown", "32", "Female", "51000", "2018-09-01")
 ]
additional_df = spark.createDataFrame(data=additional_data, schema=emp_schema)
union_df = emp_df.union(additional_df)
union_df.show()

from pyspark.sql import SparkSession
spark = ( SparkSession.builder .appName("SparkExamples") .master("local[*]") .getOrCreate())

emp_data = [1, 2, 36, 8, 2, 7, 1, 8, 2]
rdd = spark.sparkContext.parallelize(emp_data)

#map
mapped_rdd = rdd.map(lambda x: x * 2)
print(mapped_rdd.collect())

#filter
filtered_rdd = rdd.filter(lambda x: x > 4)
print(filtered_rdd.collect())

emp_data = [ ("001", "101", "Alex Murphy", "28", "Male", "52000", "2015-01-01"), ("002", "101", "Samantha Green", "31", "Female", "47000", "2016-02-15"), ("003", "102", "Chris Johnson", "37", "Male", "56000", "2014-05-01"), ("004", "102", "Rebecca Lee", "26", "Female", "49000", "2017-09-30"), ("005", "103", "Mark Davis", "42", "Male", "61000", "2013-04-01") ]
emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)
emp_df.show()

#flatmap
rddL = emp_df.rdd
flatmapped_rdd = rddL.flatMap(lambda x: x.name.split(" "))
print(flatmapped_rdd.collect())

#union
rdd2 = spark.sparkContext.parallelize([100, 200, 300])
union_rdd = rdd.union(rdd2)
print(union_rdd.collect())

#interseccion
rdd3 = spark.sparkContext.parallelize([2, 8, 10])
intersection_rdd = rdd.intersection(rdd3)
print(intersection_rdd.collect())

#distinct
distinct_rdd = rdd.distinct()
print(distinct_rdd.collect())

#gruopByKey
pair_rdd = rddL.map(lambda x: (x.departament_id, x.salary))
grouped_rdd = pair_rdd.groupByKey()
print([(x, list(y)) for x, y in grouped_rdd.collect()])

#reduceByKey
pair_rdd = rddL.map(lambda x: (x.departament_id, int(x.salary)))
reduced_rdd = pair_rdd.reduceByKey(lambda x, y: x + y)
print(reduced_rdd.collect())

#sortByKey
sorted_rdd = reduced_rdd.sortByKey()
print(sorted_rdd.collect())

#join
rdd4 = spark.sparkContext.parallelize([(2, 'a'), (8, 'b'), (36, 'c')])
joined_rdd = reduced_rdd.join(rdd4)
print(joined_rdd.collect())

#cogruop
rdd5 = spark.sparkContext.parallelize([(1, 'x'), (2, 'y'), (36, 'z')])
cogrouped_rdd = reduced_rdd.cogroup(rdd5)
print([(x, (list(y[0]), list(y[1]))) for x, y in cogrouped_rdd.collect()])

#coalesce
coalesced_rdd = rdd.coalesce(1)
print(coalesced_rdd.collect())